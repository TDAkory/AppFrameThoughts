# [Prefix Code](https://en.wikipedia.org/wiki/Prefix_code)

Prefix codes / Prefix-free codes / Instantaneous codes:

No codeword is a prefix of another codeword.

## 核心原理一：概率与码长的对应关系

如果符号 s_1 的概率 \(P(s_1) > P(s_2)\)，则其码长应满足：

\[l(s_1) \leq l(s_2)\]

即概率越大的符号，编码长度应越短。

解释：  
这是构建最优前缀码（如霍夫曼编码）的基本原则。通过将较短的码字分配给高频符号，可以最小化平均码长 \(E[l(x)]\)。如果违反此规则，可以通过交换两符号的码字来获得更短的平均码长。

## 核心原理二：最优码长的近似公式

理想情况下，单个符号的最优码长近似为：

\[l(x) \approx \log_2 \frac{1}{P(x)}\]

解释：  
该公式来源于香农信息论，表示符号 x 的最优码长应接近其“信息量”或“自信息” \(-\log_2 P(x)\)。

• 当 \(P(x) = 1\)（必然出现），\(l(x) \approx 0\)，无需编码。  

• 当 \(P(x) = 0.5\)，\(l(x) \approx 1\) 比特。  

• 当 \(P(x)\) 很小，\(l(x)\) 很大，因为罕见符号应使用较长码字。

注意：实际编码（如霍夫曼编码）的码长必须是整数，因此只能近似该理论值。

总结与应用

这两条原理共同指导如何设计最小化平均码长的前缀码：

1. 排序分配：将符号按概率降序排列，概率越大码长越短。
2. 码长目标：使每个符号的码长尽量接近 \(-\log_2 P(x)\)。
3. 优化方法：霍夫曼编码、香农-法诺编码等算法是这些原理的具体实现。

公式推导提示：  
最小化平均码长 \(E[l(X)] = \sum P(x) l(x)\)，在满足 Kraft 不等式 \(\sum 2^{-l(x)} \leq 1\) 的条件下，用拉格朗日乘子法可推导出 \(l(x) = -\log_2 P(x)\) 为理论最优解。

## 香农编码

香农编码是信息论中一种重要的**变长编码方法**，它的核心思想很直观：**为出现概率大的符号分配短的码字，为出现概率小的符号分配长的码字**，从而在整体上降低平均码长，实现高效的数据压缩。

下表清晰地展示了香农编码与其他两种常见变长编码方法的关键特性对比：

| 编码方法 | 提出者与时间 | 是否最佳码 | 核心原理 | 优点 | 缺点 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **香农编码** | 克劳德·香农 (1948) | **否** (除特定情况) | 基于符号的**累计概率** | 理论奠基，算法简单直观 | 平均码长通常非最短，效率不高 |
| **费诺编码** | 罗伯特·费诺 (1949) | **否** | **递归二分**信源，使分组概率和近似相等 | 编码效率通常优于香农编码 | 结果不唯一，受分组策略影响，可能非最优 |
| **霍夫曼编码** | 戴维·霍夫曼 (1952) | **是** (对多元独立信源) | **自底向上**构建二叉树，合并概率最小的符号 | **平均码长最短**，是最佳码 | 编码复杂度高于香农编码和费诺编码 |

### 🔢 编码步骤详解

香农编码的步骤非常规整，下面我们结合一个例子来具体说明。假设有一个信源，包含6个符号A、B、C、D、E、F，其出现概率分别为：`[0.25, 0.25, 0.2, 0.15, 0.10, 0.05]`。

1.  **排序信源符号**：首先，将信源符号按照其出现的概率从大到小进行降序排列。
    | 符号 | 概率 |
    | :--- | :--- |
    | A | 0.25 |
    | B | 0.25 |
    | C | 0.20 |
    | D | 0.15 |
    | E | 0.10 |
    | F | 0.05 |

2.  **计算码字长度**：对于每个符号，根据公式 \( l_i = \lceil -\log_2(p_i) \rceil \) 计算其码字长度，其中 \( p_i \) 是符号概率，\( \lceil \cdot \rceil \) 表示向上取整。这个公式确保码长能唯一表示该符号的信息量。
    | 符号 | 概率 | \( -\log_2(p_i) \) | 计算过程（取整） | 码长 \( l_i \) |
    | :--- | :--- | :--- | :--- | :--- |
    | A | 0.25 | \( -\log_2(0.25) = 2 \) | \( \lceil 2 \rceil \) | 2 |
    | B | 0.25 | \( -\log_2(0.25) = 2 \) | \( \lceil 2 \rceil \) | 2 |
    | C | 0.20 | \( -\log_2(0.20) \approx 2.32 \) | \( \lceil 2.32 \rceil = 3 \) | 3 |
    | D | 0.15 | \( -\log_2(0.15) \approx 2.74 \) | \( \lceil 2.74 \rceil = 3 \) | 3 |
    | E | 0.10 | \( -\log_2(0.10) \approx 3.32 \) | \( \lceil 3.32 \rceil = 4 \) | 4 |
    | F | 0.05 | \( -\log_2(0.05) \approx 4.32 \) | \( \lceil 4.32 \rceil = 5 \) | 5 |

3.  **计算累加概率**：计算每个符号的累加概率（也称累积概率）\( P_i \)。第一个符号的累加概率为0，后续符号的累加概率等于它前面所有符号的概率之和。
    | 符号 | 概率 | 累加概率 \( P_i \) |
    | :--- | :--- | :--- |
    | A | 0.25 | 0.00 |
    | B | 0.25 | 0.25 (A的概率) |
    | C | 0.20 | 0.50 (A+B的概率) |
    | D | 0.15 | 0.70 (A+B+C的概率) |
    | E | 0.10 | 0.85 (A+B+C+D的概率) |
    | F | 0.05 | 0.95 (A+B+C+D+E的概率) |

4.  **将累加概率转换为二进制码字**：将每个符号的累加概率 \( P_i \) 转换为二进制小数，然后取该二进制小数的小数点后前 \( l_i \) 位（即码长位）作为该符号的最终码字。
    | 符号 | 概率 | 码长 \( l_i \) | 累加概率 \( P_i \) | \( P_i \) (二进制) | 取前 \( l_i \) 位作为码字 |
    | :--- | :--- | :--- | :--- | :--- | :--- |
    | A | 0.25 | 2 | 0.00 | 0.00 | **00** |
    | B | 0.25 | 2 | 0.25 | 0.01 | **01** |
    | C | 0.20 | 3 | 0.50 | 0.100 | **100** |
    | D | 0.15 | 3 | 0.70 | 0.1011001... | **101** |
    | E | 0.10 | 4 | 0.85 | 0.110110011... | **1101** |
    | F | 0.05 | 5 | 0.95 | 0.111100111... | **11110** |

    最终，我们得到所有符号的香农码字。

### 💡 核心原理与评价

- **理论基础**：香农编码的理论基础是香农第一定理（可变长无失真信源编码定理）。该定理指出，在对信源符号进行编码时，每个符号的平均码长其极限值就是信源的**熵** (Entropy)。香农编码是这一理论最直接的体现。
- **即时码**：香农编码产生的码是**即时码**（也称前缀码），即任何一个码字都不是另一个码字的前缀。这使得在解码时，一旦识别出一个完整的码字，就可以立即翻译它，无需等待后续码字，解码非常高效且无歧义。
- **编码效率**：虽然香农编码在理论上有重要的指导意义，但在实际应用中，其**编码效率通常不是最高的**，即它的平均码长一般**不是最短的**。相比之下，霍夫曼编码通常能获得更短的平均码长，是更优的实用选择。

## Kraft不等式

Kraft不等式是信息论和编码理论中一个**基础且重要的数学关系**，它为特定码字长度集合能否构成某类有效编码提供了判断准则。下表概括了其核心要点与应用。

| 特性维度 | 核心描述 |
| :--- | :--- |
| **基本形式** | 对于 D 元字母表，若一组码字长度为 $l_1, l_2, ..., l_m$，则存在对应前缀码的**充要条件**是满足 $\sum_{k=1}^{m} D^{-l_k} \leq 1$。 |
| **适用范围** | 该不等式最初针对**前缀码**（即时码）提出，但麦克米兰证明它也适用于所有**唯一可译码**。这意味着，唯一可译码的码长也必须满足此不等式。 |
| **不等式状态** | **和小于1**：编码存在**冗余**。<br>**和等于1**：编码是**完备码**，所有可能的码字组合都被使用，没有浪费。<br>**和大于1**：**不可能**构造出唯一可译码。 |
| **核心价值** | 它建立了码字长度与编码存在性之间的桥梁，是判断能否构造出特定编码的**理论基石**。 |

### 🔢 直观理解：D叉树模型

理解Kraft不等式最直观的方式是借助**D叉树**的比喻。
-   想象一棵树，每个节点最多有 **D 个子分支**（对应D元字母表，如二进制D=2）。
-   树的**根节点**位于第0层。
-   一个长度为 $l_i$ 的码字，可以看作是从根节点出发，沿着树枝走 $l_i$ 步后到达的一个**叶子节点**。
-   **前缀码的关键规则**是：**任何一个码字对应的节点，都不能是另一个更长码字的祖先节点**。也就是说，一旦某个节点被选为码字，它的所有后代节点就都不能再被用作其他码字了。

在这个模型下，一个长度为 $l_i$ 的码字，实际上"独占"了以它为根的一棵子树，这棵子树在最深层（假设最大码长为 $l_{max}$）拥有 $D^{l_{max} - l_i}$ 个叶子节点。所有码字所"独占"的叶子节点总数，不能超过整棵树在最深层所能拥有的叶子节点总数 $D^{l_{max}}$。将不等式两边同时除以 $D^{l_{max}}$，就得到了Kraft不等式 $\sum D^{-l_i} \leq 1$。

证明思路的精髓：将码字空间想象成一棵完整的二叉树，每个码字都对应树上的一个节点。构建前缀码的过程，就像在树上“圈地”，而Kraft不等式就是衡量“所有圈地面积之和不能超过整棵树”的数学表达。

### 💡 主要应用场景
1.  **判断编码的存在性**：这是Kraft不等式最直接的应用。在设计变长编码（如霍夫曼编码）时，在确定码长后，可以先用Kraft不等式验证是否存在对应的前缀码。如果不满足不等式，则需调整码长。
2.  **评估编码效率**：不等式求和的结果（冗余度）可以衡量编码的紧凑程度。当求和等于1时（完备码），意味着编码空间被完全利用，没有冗余，效率最高。
3.  **关联最优码长**：在寻找平均码长最短的最优编码时，Kraft不等式常作为约束条件出现在优化问题中。理论上，最优码长 $l_i^*$ 与符号概率 $p_i$ 的理想关系是 $l_i^* = -\log_D p_i$，此时平均码长等于信源的熵 $H_D(X)$，达到了理论下限。

## 熵

一、熵的定义与公式

熵（Entropy）是衡量一个随机变量不确定性的度量，单位是比特。

• 设随机变量 X 有 k 个可能的取值：\{1, 2, \ldots, k\}

• 每个取值出现的概率为：\(p_i = P(X = i)\)

熵的公式为：

$H(X) = \sum_{i=1}^{k} p_i \log_2 \frac{1}{p_i}$

理解：公式中 \log_2 \frac{1}{p_i} 可以看作取值 i 发生时带来的“惊喜度”或“信息量”。概率 p_i 越小，其倒数 1/p_i 越大，取对数后得到的信息量越大。熵 \(H(X)\) 就是所有可能取值所带来的“平均信息量”或“平均不确定程度”。

1. H(x) is really H(P)
   1. 含义：熵 \(H(X)\) 本质上刻画的是随机变量 X 的概率分布 P 的特性，而不是 X 的具体取值。即使随机变量 X 和 Y 的取值范围完全不同，只要它们的概率分布形状相同，它们的熵就相同。
   2. 例子：一个公平硬币（正面/反面）和一个公平骰子（1/2/3/4/5/6）的熵是不同的，因为概率分布不同。但一个质量不均的硬币（正:90%，反:10%）和一个质量不均的骰子（某个面:90%，其他五面各:2%）如果概率分布形态（不确定度）相似，则熵也相似。

2. H(x)= E{ log₂ 1/P(x) }
   1. 含义：这是熵的期望形式。它表明熵是函数 \(\log_2 \frac{1}{P(X)}\) 的数学期望。这直接将熵定义为“每个事件信息量的平均值”，是公式的另一种等价且更本质的表达。
   2. $H(X) = \sum_i p_i \log_2 \frac{1}{p_i} = E\left[ \log_2 \frac{1}{P(X)} \right]$

3. p=0: 我认为它在概念上非常相似
   1. 背景：在熵的原始定义中，当某个 p_i = 0 时，项 0 \cdot \log_2 \frac{1}{0} 是未定义的。但这在信息论中是标准处理。
   2. 解释：我们约定 0 \cdot \log_2 \frac{1}{0} = 0。其概念依据是：一个概率为0的事件如果发生，会带来无限大的“惊喜”（因为 1/0 \to \infty），但它永远不会发生（乘以0），所以它对平均不确定性的贡献为0。这与极限概念一致：\(\lim_{p \to 0^+} p \log_2(1/p) = 0\)。

熵是信息论的基石，它给出了无损压缩数据的理论极限：

- 对随机变量 X 进行编码，其平均码长的理论最小值就是 \(H(X)\) 比特。
- 之前的香农编码、霍夫曼编码等，其目标就是尽可能接近这个下限。
- 同时，熵也为衡量信源的信息含量、通信信道的容量提供了基本度量。

**对于一个定义在大小为 k 的字母表上的随机变量 X，其熵 \(H(X)\) 满足：$0 \leq H(X) \leq \log_2 k$**

熵度量的是随机变量的“不确定性”或“信息含量”。这个性质表明：

- 熵值越低，意味着随机变量的分布越集中，不确定性越小，越容易预测。
- 熵值越高，意味着分布越平均，不确定性越大，越难以预测。

例子：

- 一个以99.9%概率正面朝上的硬币，其熵接近于0。
- 一枚公平硬币（正反面各50%），其熵为 H = \log_2 2 = 1 比特。
- 一个均匀的六面骰子，其熵为 H = \log_2 6 \approx 2.585 比特。

## 无损压缩

一个**下界**和一个**可达性声明**，共同构成了无损压缩的完整理论图景：

**下界定理：熵是绝对极限**

对于任何一个唯一可译码（课件中特指**前缀码**），其平均码长 \(E[l(x)]\) 必定**大于或等于**信源的熵 \(H(X)\)。

$E[l(x)] \geq H(X)$

**可达性定理：熵是可达的极限**
    
我们可以设计出前缀码，使得其平均码长 \(E[l(x)]\) 无限接近熵 \(H(X)\)。

$E[l(x)] \approx H(X)$，并且可以**任意逼近**。

这两个结论互为补充，共同定义了熵的根本角色：

* **下界定理（第一部分）** 告诉你**不可能比熵做得更好**。它回答了“压缩的极限在哪里？”——答案就是**熵**。
* **可达性定理（第二部分）** 告诉你**可以达到这个极限**。它回答了“我们能否做到极限？”——答案是**可以无限逼近**。

因此，**熵 \(H(X)\) 是数据压缩的“绝对极限”和“终极目标”**。
